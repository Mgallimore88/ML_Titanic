{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic competition quick entry creation\n",
    "\n",
    "Notebook for creating quick competition entries for the Titanic kaggle competition - this should be a concise way of training a model and outputting the csv files without the analysis carried out in the first two notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entry removes the independent variables which make the smallest or no influence on the depen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_cols.npy  titanic_procdfd_raw  titanic_raw_cats\r\n"
     ]
    }
   ],
   "source": [
    "PATH = \"data/titanic\"\n",
    "!ls tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"data/titanic/\"\n",
    "max_n_cat = 5\n",
    "df_raw = pd.read_feather('tmp/titanic_raw_cats')\n",
    "df, y, nas = proc_df(df_raw, 'Survived', max_n_cat=max_n_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a,n): \n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "# n_valid = 418 #same as Kaggle's test set size\n",
    "n_valid = 100 #smaller validation set to provide more training data\n",
    "n_trn = len(df)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "x_train, x_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "raw_train.shape, x_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred,actual): return math.sqrt(((pred-actual)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(m):\n",
    "    result = [f'rmse train: {rmse(m.predict(x_train), y_train)} Training set size: {len(y_train)}', \n",
    "              f'\\nrmse validation: {rmse(m.predict(x_valid), y_valid)} Validation set size: {len(y_valid)}',\n",
    "              f'\\nRsquared train: {m.score(x_train, y_train)}',\n",
    "              f'\\nRSquared valid: {m.score(x_valid, y_valid)}']\n",
    "    print(result[0], result[1], result[2], result[3])\n",
    "    if hasattr(m, 'oob_score_'):\n",
    "        result.append(f'OOB: {m.oob_score_}')\n",
    "        print(result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_print_score(m, x_t, y_t):\n",
    "    result = [f'rmse train: {rmse(m.predict(x_t), y_t)} Total set size: {len(y_t)}', \n",
    "              f'\\nRsquared train: {m.score(x_t, y_t)}']\n",
    "    print(result[0], result[1])\n",
    "    if hasattr(m, 'oob_score_'):\n",
    "        result.append(f'OOB: {m.oob_score_}')\n",
    "        print(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \n",
    "                           \"display.max_columns\", 1000):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dectree_max_depth(tree):\n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "    \n",
    "\n",
    "    def walk(node_id):\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            left_max = 1 + walk(children_left[node_id])\n",
    "            right_max = 1 + walk(children_right[node_id])\n",
    "            return max(left_max, right_max)\n",
    "        else: # leaf\n",
    "            return 1\n",
    "\n",
    "    root_node_id = 0\n",
    "    return walk(root_node_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(700) # Number of passengers available to each tree \n",
    "# reset_rf_samples() # Set each tree to have access to all rows\n",
    "n_estimators = 400 # Number of trees/estimators in model\n",
    "min_samples_leaf = 3 # number remaining in each leaf node - average taken\n",
    "max_features = 0.6 # For each decision , proportion of independent variables available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set and validation set scores\n",
    "m = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_features=max_features, n_jobs=-1, oob_score=True)\n",
    "m.fit(x_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = rf_feat_importance(m,df);\n",
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide which variables to keep and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['PassengerId'] # from our tests in L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = feature_importance[feature_importance.imp>0.05].cols\n",
    "df_keep = df[to_keep].copy()\n",
    "df_keep.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tmp/keep_cols.npy', np.array(df_keep.columns))\n",
    "df_keep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=m.estimators_[0].tree_\n",
    "print(f'This tree has {dectree_max_depth(tree)} splits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model on the combined set of training and validation data with reduced number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(700) # Number of passengers available to each tree \n",
    "# reset_rf_samples() # Set each tree to have access to all rows\n",
    "n_estimators = 1000 # Number of trees/estimators in model\n",
    "min_samples_leaf = 3 # number remaining in each leaf node - average taken\n",
    "max_features = 0.6 # For each decision , proportion of independent variables available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_features=max_features, n_jobs=-1, oob_score=True)\n",
    "m.fit(df_keep, y)\n",
    "custom_print_score(m, df_keep, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv('data/titanic/test.csv')\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, _ , nas_test = proc_df(testset, max_n_cat=max_n_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_keep = df_test[df_keep.columns]\n",
    "# df_test_keep.drop(to_drop, axis=1, inplace=True)\n",
    "df_test_keep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the testset using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.predict(df_test_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average predicted survival rate in the test cases\n",
    "m.predict(df_test_keep).sum()/len(m.predict(df_test_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = [int(x) for x in (predictions.round())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_keep.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the prediction data to a .CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"PassengerId\" :df_test_keep.index+892, \"Survived\": binary_predictions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('second_titanic_submission.csv', index=False)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
